import numpy as np
import pandas as pd
import os
import tensorflow as tf
from tensorflow import keras
print(tf.__version__)
%%%


'''
1、preprocessing
2、build model
2.1、encoder
2.2、attention
2.3、decoder
3、evaluation
3.1、give sentence, return translated results
3.2、visualize results(attention)
'''
%%%


en_spa_file_path = './data_spa_en/spa.txt'
​
# 1、西班牙语中有些特殊字符是unicode表示，需要转为ascii码
# 2、可以减小属性量
import unicodedata
def unicode_to_ascii(s):
    return ''.join(c for c in unicodedata.normalize('NFD', s)
                   if unicodedata.category(c) != 'Mn')
en_sentence = 'Then what?'
sp_sentence = ''
​
print(unicode_to_ascii(en_sentence))
print(unicode_to_ascii(sp_sentence))
%%%

# 将标点符号和单词分开，再去掉多余的空格
import re
def preprocess_sentence(s):
    s = unicode_to_ascii(s.lower().strip())
    
    # 标点前后加空格
    s = re.sub(r"([?.!,])", r" \1 ", s)
    # 多个空格变成一个空格
    s = re.sub(r'[" "]+', " ", s)
    
    # 将字符串中除了字母和标点外的所有字符替换成空格
    s = re.sub(r'[^a-zA-Z?.!,]', " ", s)
    # 去掉前后的空格
    s = s.rstrip().strip()
    
    # 添加特殊字符
    s = '<start>' + s + '<end>'
    return s
​
print(preprocess_sentence(en_sentence))
print(preprocess_sentence(sp_sentence))
%%%

# 读取数据
def parse_data(filename):
    # 读取行
    lines = open(filename, encoding = 'utf-8').read().strip().split('\n')
    # 分割英语和西班牙语
    sentence_pairs = [line.split('\t') for line in lines]
    preprocessed_sentence_pairs = [
        (preprocess_sentence(en), preprocess_sentence(sp)) for en, sp in sentence_pairs]
    return zip(*preprocessed_sentence_pairs)
​
en_dataset, sp_dataset = parse_data(en_spa_file_path)
print(en_dataset[-1])
print(sp_dataset[-1])
%%%


a = [(1, 2), (3, 4), (5, 6)]
c, d = zip(*a)
print(c, d)
%%%


# char2id
def tokenizer(lang):
    lang_tokenizer = keras.preprocessing.text.Tokenizer(
        num_words = None, filters = '', split = ' ')
    # 统计词频，生成词表
    lang_tokenizer.fit_on_texts(lang)
    # 文本转id
    tensor = lang_tokenizer.texts_to_sequences(lang)
    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')
    return tensor, langu_tokenizer
​
# 对数据集做转换
input_tensor, input_tokenizer = tokenizer(sp_dataset[0:30000])
output_tensor, output_tokenizer = tokenizer(en_dataset[0:30000])
​
def max_length(tensor):
  return max(len(t) for t in tensor)

max_length_input = max_length(input_tensor)
max_length_output = max_length(output_tensor)
print(max_length_input, max_length_output)
%%%

from sklearn.model_selection import train_test_split
input_train, input_eval, output_train, output_eval = train_test_split(input_tensor, output_tensor, test_size = 0.2)
print(len(input_train), len(input_eval), len(output_train), len(output_eval))
%%%

# 检测tokenizer的转换是否正确
def convert(example, tokenizer):
  for t in example:
    if t != 0:
      print('%d --> %s' % (t, tokenizer.index_word[t])) # 得到字符串
      
convert(input_train[0], input_tokenizer)
print()
convert(output_train[0], output_tokenizer)
%%%

# 生成dataset
def make_dataset(input_tensor, output_tensor, batch_size, epochs, shuffle):
  dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor))
  if shuffle:
    dataset = dataset.shuffle(30000)
  dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder = True)
  return dataset
  
batch_size = 64
epochs = 20
train_dataset = make_dataset(input_train, output_train, batch_size, epochs, True)
eval_dataset = make_dataset(input_eval, output_eval, batch_size, 1, False)

for x, y in train_dataset.take(2):
  print(x.shape)
  print(y.shape)
  print(x)
  print(y)
%%%

# 定义模型
embedding_units = 256
units = 1024
# word_index是一个字典
input_vocab_size = len(input_tokenizer.word_index) + 1
output_vocab_size = len(output_tokenizer.word_index) + 1
%%%

class Encoder(keras.Model):
    def __init__(self, vocab_size, embedding_units, encoding

















