{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "1、preprocessing\n",
    "2、build model\n",
    "2.1、encoder\n",
    "2.2、attention\n",
    "2.3、decoder\n",
    "2.4、loss & optimizer\n",
    "2.5、train\n",
    "3、evaluation\n",
    "3.1、give sentence, return translated results\n",
    "3.2、visualize results(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_spa_file_path = './data_spa_en/spa.txt'\n",
    "\n",
    "# 1、西班牙语中有些特殊字符是unicode表示，需要转为ascii码\n",
    "# 2、可以减小属性量\n",
    "import unicodedata\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "en_sentence = 'Then what?'\n",
    "sp_sentence = ''\n",
    "\n",
    "print(unicode_to_ascii(en_sentence))\n",
    "print(unicode_to_ascii(sp_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将标点符号和单词分开，再去掉多余的空格\n",
    "import re\n",
    "def preprocess_sentence(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    \n",
    "    # 标点前后加空格\n",
    "    s = re.sub(r\"([?.!,])\", r\" \\1 \", s)\n",
    "    # 多个空格变成一个空格\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    \n",
    "    # 将字符串中除了字母和标点外的所有字符替换成空格\n",
    "    s = re.sub(r'[^a-zA-Z?.!,]', \" \", s)\n",
    "    # 去掉前后的空格\n",
    "    s = s.rstrip().strip()\n",
    "    \n",
    "    # 添加特殊字符\n",
    "    s = '<start>' + s + '<end>'\n",
    "    return s\n",
    "\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def parse_data(filename):\n",
    "    # 读取行\n",
    "    lines = open(filename, encoding = 'utf-8').read().strip().split('\\n')\n",
    "    # 分割英语和西班牙语\n",
    "    sentence_pairs = [line.split('\\t') for line in lines]\n",
    "    preprocessed_sentence_pairs = [\n",
    "        (preprocess_sentence(en), preprocess_sentence(sp)) for en, sp in sentence_pairs]\n",
    "    return zip(*preprocessed_sentence_pairs)\n",
    "\n",
    "en_dataset, sp_dataset = parse_data(en_spa_file_path)\n",
    "print(en_dataset[-1])\n",
    "print(sp_dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [(1, 2), (3, 4), (5, 6)]\n",
    "c, d = zip(*a)\n",
    "print(c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char2id\n",
    "def tokenizer(lang):\n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "        num_words = None, filters = '', split = ' ')\n",
    "    # 统计词频，生成词表\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    # 文本转id\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n",
    "    return tensor, langu_tokenizer\n",
    "\n",
    "# 对数据集做转换\n",
    "input_tensor, input_tokenizer = tokenizer(sp_dataset[0:30000])\n",
    "output_tensor, output_tokenizer = tokenizer(en_dataset[0:30000])\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "max_length_input = max_length(input_tensor)\n",
    "max_length_output = max_length(output_tensor)\n",
    "print(max_length_input, max_length_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_train, input_eval, output_train, output_eval = train_test_split(input_tensor, output_tensor, test_size = 0.2)\n",
    "print(len(input_train), len(input_eval), len(output_train), len(output_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检测tokenizer的转换是否正确\n",
    "def convert(example, tokenizer):\n",
    "    for t in example:\n",
    "        if t != 0:\n",
    "            print('%d --> %s' % (t, tokenizer.index_word[t]))# 得到字符串\n",
    "\n",
    "convert(input_train[0], input_tokenizer)\n",
    "print()\n",
    "convert(output_train[0], output_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成dataset\n",
    "def make_dataset(input_tensor, output_tensor, batch_size, epochs, shuffle):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(30000)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder = True)\n",
    "    return dataset\n",
    "  \n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "train_dataset = make_dataset(input_train, output_train, batch_size, epochs, True)\n",
    "eval_dataset = make_dataset(input_eval, output_eval, batch_size, 1, False)\n",
    "\n",
    "for x, y in train_dataset.take(2):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "embedding_units = 256\n",
    "units = 1024\n",
    "# word_index是一个字典\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
    "        self.gru = keras.layers.GRU(self.encoding_units,\n",
    "                                    return_sequences = True,\n",
    "                                    return_state = True,\n",
    "                                    recurrent_initializer = 'glorot_uniform')\n",
    "    \n",
    "    # hidden:初始化的隐含状态\n",
    "    def call(self, x, hidden):\n",
    "        # 直接调用self.embedding()，将输入嵌入\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "    \n",
    "    # 创建隐含状态，传给call函数\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros(slef.batch_size, self.encoding_units)\n",
    "    \n",
    "encoder = Encoder(input_vocab_size, embedding_units, units, batch_size)\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "# 执行call函数\n",
    "sample_output, sample_hidden = encoder(x, sample_hidden)\n",
    "\n",
    "print(\"sample_output.shape:\", sample_output.shape)\n",
    "print(\"sample_hidden_shape\", sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention机制\n",
    "class BahdanauAttention(keras.Model):\n",
    "    def __init__(self, units):  # 这里的units是指经过中间经过w1，w2之后的输入维度\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        # 参看笔记本中的公式\n",
    "        self.w1 = keras.layers.Dense(units)\n",
    "        self.w2 = keras.layers.Dense(units)\n",
    "        self.v = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hidden.shape = (batch_size, units) 这里的units不同于__init__中的units\n",
    "        # encoder_outputs: (batch_size, length, units) 这里的事encoder和decoder中units数目\n",
    "        decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
    "        \n",
    "        # 实现attention方法\n",
    "        # 注意: before self.v  (batch_size, length, units)\n",
    "        #      after          (batch_size, length, 1)\n",
    "        score = slef.v(\n",
    "                tf.nn.tanh(\n",
    "                    self.w1(encoder_outputs) + self.w2(decoder_hidden_with_time_axis)))\n",
    "        # shape:(batch_size, length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis = 1)  # softmax的维度是在length维度上\n",
    "        \n",
    "        # 默认将第三个维度上的1广播为units，方便进行element-wise的乘法\n",
    "        # shape: (batch_size, length, units)\n",
    "        context_vector = attention_weights * encoder_outputs \n",
    "        # shape: (batch_size, units)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
    "        \n",
    "        # attention_weights为了做可视化\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "attention_model = BahdanauAttention(units = 10)\n",
    "attention_results, attention_weights = attention_model(sample_hidden, sample_output)\n",
    "print(\"attention_results.shape:\", attention_results.shape)\n",
    "print(\"attention_weights.shape:\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, decoding_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
    "        self.gru = keras.layers.GRU(self.decoding_units, \n",
    "                                    return_sequences = True,\n",
    "                                    return_state = True,\n",
    "                                    recurrent_intializer = 'glorot_uniform')\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        self.attention = BahdanauAttention(self.decoding_units)\n",
    "    \n",
    "    def call(self, x, hidden, encoding_outputs):\n",
    "        # context_vector.shape: (batch_size, units)\n",
    "        context_vector, attention_weights = slef.attention(hidden,encoding_units)\n",
    "        \n",
    "        # before embedding:shape = (batch_size, 1)\n",
    "        # after embedding: shape = (batch_size, 1, embedding_units)\n",
    "        x = slef.embedding(x)\n",
    "        \n",
    "        # 让x与context_vector拼接，共同输入给gru\n",
    "        conbined_x = tf.concat(\n",
    "            [tf.expand_dims(context_vector, 1), x], axis = -1)\n",
    "        \n",
    "        # output_shape = (batch_size, 1, decoding_units)\n",
    "        # state_shape = (batch_size, decoding_units)\n",
    "        output, state = self.gru(conbined_x)\n",
    "        \n",
    "        # output_shape = (batch_size, decoding_units)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output_shape = (batch_size, vocab_size) \n",
    "        output = self.fc(output)\n",
    "        return output, state, attetion_weights\n",
    "    \n",
    "decoder = Decoder(output_vocab_size, embedding_units, units, batch_size)\n",
    "outputs = decoder(tf.random.uniform((batch_size, 1)), sample_hidden, sample_output)\n",
    "decoder_output, decoder_hidden, decoder_aw = outputs\n",
    "print(\"decoder_output.shape:\", decoder_output.shape)\n",
    "print(\"decoder_hidden.shape:\", decoder_hidden.shape)\n",
    "print(\"decoder_attention_weights.shape:\", decoder_aw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数和optimizer\n",
    "# 因为模型中FC的输入没有经过activation，from_logits = True\n",
    "# reduction:定义了损失函数如何聚合\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n",
    "optimizer  = keras.optimizer.Adam()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # 使padding对应的损失函数为0\n",
    "    # mask = False,  id = 0; = True, id != 0\n",
    "    mask = tf.math.logical_not(tf.math.euqal(real, 0))  # 布尔型\n",
    "    loss_ = loss(real, pred)   # float\n",
    "    mask = tf.cast(mask, dtype = loss_.dtype)\n",
    "    \n",
    "    loss_ *= mask\n",
    "    # 在loss乘完mask之后再做聚合\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "# 计算多步的损失函数，然后做梯度下降\n",
    "def train_step(inp, targ, encoding_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        # encoding_hidden 就是初始状态\n",
    "        encoding_outputs, encoding_hidden = encoder(inp, encoding_hidden)\n",
    "        decoding_hidden = encoding_hidden\n",
    "        \n",
    "        # eg: <start> I am here <end>\n",
    "        # 句子长度为5，需要遍历4次\n",
    "        # 1、 <start> -> I\n",
    "        # 2、 I -> am\n",
    "        # 3、 am -> here\n",
    "        # 4、 here -> <end>\n",
    "        for i in range(0, targ.shape[1] - 1):\n",
    "            decoding_input = tf.expand_dims(targ[:, t], 1)\n",
    "            \n",
    "            predictions, decoding_hidden, aws = decoder(\n",
    "                decoding_input, decoding_hidden, encoding_outputs)\n",
    "            loss += loss_functin(targ[:, t+1], predictions)\n",
    "    # 每个batch平均的损失函数,为了避免不同的batch_size造成结果差异\n",
    "    batch_loss = loss / int(targ.shape[0])\n",
    "    # 两个列表合并\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    # 这里batch_loss和loss都行\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "epochs = 10\n",
    "steps_per_epoch = len(input_tensor) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    encoding_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, encoding_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, batch_loss.numpy()))\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "    print('Time take for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_tensor):\n",
    "    # 笔记的图显示了维度\n",
    "    attenion_matrix = tf.zeros((max_length_output, max_length_output))\n",
    "    input_tensor = preprocess_sentence(input_tensor)\n",
    "    \n",
    "    inputs = [input_tokenizer.word_index[token] for token in input_tensor.split(' ')]\n",
    "    inputs = keras.preprocessing.sequence.pad_sequences(\n",
    "        [inputs], maxlen = max_length_input, padding = 'post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    results = ''\n",
    "    # 报错中：encoding_hidden = encoder.initialize_hidden_state()\n",
    "    encoding_hidden = tf.zeros((1, units))\n",
    "    \n",
    "    encoding_outputs, encoding_hidden = encoder(inputs, encdoing_hideen)\n",
    "    decoding_hidden = encoding_hidden\n",
    "    \n",
    "    # 上一步预测值作为下一步的输入\n",
    "    # shape = (1, 1)\n",
    "    decoding_input = tf.expand_dims([\n",
    "        output_tokenizer.word_index['<start>']], 0)\n",
    "    for t in range(max_length_output):\n",
    "        predictions, decoding_hidden, attention_weights = decoder(\n",
    "            decoding_inputs, decoding_hidden, encoding_outputs)\n",
    "        # attention_weights shape = (batch_size, input_length, 1) --> (1, 16, 1)\n",
    "        # 要存入attention_matrix 需要转变成向量，把前后的两个维度消掉\n",
    "        attention_weights = tf.reshape(attention_weights, (-1,))\n",
    "        attention_matrix[t] = attention_weights.numpy()\n",
    "        \n",
    "        # 获取predictions中概率值最大的元素，作为下一步的输入\n",
    "        # predictions.shape = (batch_size, vocab_size) --> (1, 4935)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        # 将id对应的字符串加到results中\n",
    "        results += output_tokenizer.word_index[predicted_id] + ' '\n",
    "        \n",
    "        if output_tokenizer.word_index[predicted_id] == '<end>':\n",
    "            return results, input_sentence, attention_matrix\n",
    "        decoding_input = tf.expand_dims([predited_id], 0)\n",
    "    return results, input_sentence, attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化attention_matrix\n",
    "# 吴恩达视频推荐论文\n",
    "def plot_attention(attention_matrix, input_sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize = (10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention_matrix, cmap = 'viridis')\n",
    "    \n",
    "    font_dict = {'font_size': 14}\n",
    "    ax.set_xticklabels([''] + input_sentence, font_dict = font_dict, rotation = 90)\n",
    "    ax.set_yticklabels([''] + preticted_sentence, font_dict = font_dict)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sentence):\n",
    "    results, input_sentence, attention_matrix = evaluate(input_sentence)\n",
    "    \n",
    "    print(\"Input: %s\" % (input_sentence))\n",
    "    print(\"Output: %s\" % (results))\n",
    "    \n",
    "    # 对attention_matrix做预处理，使得padding部分不用打出来\n",
    "    # 让results中没有达到max_length_output的部分也去掉\n",
    "    attention_matrix = attention_matrix[:len(results.split(' ')),\n",
    "                                        :len(input_sentence.split(' '))]\n",
    "    plot_attention(attention_matrix, input_sentence.split(' '), results.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "报错：\n",
    "Invalid input_h shape: 一般input_h是计算过程中的hidden_state\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
